#!/bin/bash

# 1. ê²½ë¡œ ë° ê¸°ë³¸ í™˜ê²½ ì„¤ì •
PROJECT_ROOT="/home/dongwoo38/PiSSA"
cd $PROJECT_ROOT

BASE_MODEL="meta-llama/Llama-2-7b-hf"
DATA_PATH="fxmeng/pissa-dataset"
SEED=42

# 2. LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° DType ì„¤ì •
# ë¹„êµë¥¼ ìœ„í•´ LAVAì™€ ë™ì¼í•œ RANKë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
RANK=128
ALPHA=128  # ì¼ë°˜ LoRA/PiSSAëŠ” alpha=r ë˜ëŠ” alpha=2rì„ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
INIT_TYPE="gaussian" # "pissa" ë˜ëŠ” "gaussian" (ì¼ë°˜ LoRA) ì¤‘ ì„ íƒ

BASE_DTYPE="int4"
ADAPTER_DTYPE="fp32"

# 3. ì¶œë ¥ ê²½ë¡œ ë° WandB ì´ë¦„ ì„¤ì • (VIB ê´€ë ¨ ì§€í‘œ ì œê±°)
WANDB_NAME="[BASELINE]${INIT_TYPE^^}_Llama2_7B_r${RANK}_a${ALPHA}_B-${BASE_DTYPE}_seed${SEED}"
OUTPUT_PATH="output/metamath-${INIT_TYPE}-Llama2-r${RANK}-a${ALPHA}-seed${SEED}"

# 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export WANDB_PROJECT=NLG-comparison-baselines
export WANDB_NAME=$WANDB_NAME

# ë¶„ì‚° í•™ìŠµ ë° CUDA ì´ìŠˆ ë°©ì§€ ì„¤ì •
export NCCL_DEBUG=WARN
export NCCL_IB_DISABLE=1
export NCCL_IGNORE_DISABLED_P2P=1
export DS_SKIP_CUDA_CHECK=1 

# ìë™ í¬íŠ¸ í• ë‹¹ ë¡œì§
while true; do
    RANDOM_PORT=$(shuf -i 10000-60000 -n 1)
    if ! ss -ant | grep -q ":$RANDOM_PORT "; then
        export MASTER_PORT=$RANDOM_PORT
        break
    fi
done

echo "Using Master Port: $MASTER_PORT"

# 5. DeepSpeed ì‹¤í–‰ ëª…ë ¹ì–´
# ğŸš€ ìˆ˜ì • í¬ì¸íŠ¸: 
# - init_weightsë¥¼ ë¬¸ìì—´ "pissa" ë˜ëŠ” "gaussian"ìœ¼ë¡œ ëª…ì‹œ (True ì—ëŸ¬ ë°©ì§€)
# - lambda_vib, lambda_stab ë“± LAVA ì „ìš© ì¸ì ì „ë©´ ì œê±°
# - GPU 2ê°œë¥¼ ì‚¬ìš©í•œë‹¤ë©´ --include=localhost:0,1 ë¡œ ì„¤ì •í•˜ì„¸ìš”.
deepspeed --master_port=$MASTER_PORT --include=localhost:0,1 train.py \
  --deepspeed configs/ds_config_zero2_no_offload.json \
  --full_finetune False \
  --model_name_or_path $BASE_MODEL \
  --seed $SEED \
  --data_seed $SEED \
  --base_dtype $BASE_DTYPE \
  --adapter_dtype $ADAPTER_DTYPE \
  --init_weights $INIT_TYPE \
  --lora_rank $RANK \
  --lora_alpha $ALPHA \
  --lora_dropout 0 \
  --target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \
  --data_path $DATA_PATH \
  --sub_task metamath:100000 \
  --dataset_split train \
  --dataset_field instruction output \
  --output_dir $OUTPUT_PATH \
  --num_train_epochs 1 \
  --model_max_length 512 \
  --per_device_train_batch_size 4 \
  --gradient_checkpointing True \
  --gradient_accumulation_steps 16 \
  --learning_rate 2e-5 \
  --warmup_ratio 0.03 \
  --lr_scheduler_type cosine \
  --logging_steps 1 \
  --save_strategy steps \
  --save_steps 1000 \
  --save_total_limit 1 \
  --report_to wandb \
  --optim adamw_torch \
  --merge False