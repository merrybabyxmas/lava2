#!/bin/bash

# 1. ê²½ë¡œ ë° ê¸°ë³¸ í™˜ê²½ ì„¤ì •
PROJECT_ROOT="/home/dongwoo38/PiSSA"
cd $PROJECT_ROOT

BASE_MODEL="meta-llama/Llama-2-7b-hf"
DATA_PATH="fxmeng/pissa-dataset"
SEED=42

# 2. LAVA í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° DType ì„¤ì •
LAMBDA_VIB=1.0
LAMBDA_STAB=0.1
RANK=128
ALPHA=16

# r=8, alpha=4 ê¸°ì¤€ ìµœì  Boundì¸ 16 ì„¤ì • (í˜¹ì€ ë¹„ì›Œë‘ì–´ ìë™ê³„ì‚° ìœ ë„)

BASE_DTYPE="int4"
ADAPTER_DTYPE="fp32"

# 3. ì¶œë ¥ ê²½ë¡œ ë° WandB ì´ë¦„ì— Alpha ì¶”ê°€
# ì´ë¦„ ì˜ˆì‹œ: LAVA_Llama2_7B_r128_a16_B-int4_vib0.0005
WANDB_NAME="[CONVO]LAVA_Llama2_7B_r${RANK}_a${ALPHA}_B-${BASE_DTYPE}_A-${ADAPTER_DTYPE}_vib${LAMBDA_VIB}_seed${SEED}"
OUTPUT_PATH="output/conversation-LAVA-mistral-r${RANK}-B_${BASE_DTYPE}-A_${ADAPTER_DTYPE}-vib${LAMBDA_VIB}-seed${SEED}"

# 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export WANDB_PROJECT=NLG-conversation-ver3
export WANDB_NAME=$WANDB_NAME

# ë¶„ì‚° í•™ìŠµ ë° CUDA ì´ìŠˆ ë°©ì§€ ì„¤ì •
export NCCL_DEBUG=WARN
export NCCL_IB_DISABLE=1
export NCCL_IGNORE_DISABLED_P2P=1
export DS_SKIP_CUDA_CHECK=1 

# ìë™ í¬íŠ¸ í• ë‹¹ ë¡œì§
while true; do
    RANDOM_PORT=$(shuf -i 10000-60000 -n 1)
    if ! ss -ant | grep -q ":$RANDOM_PORT "; then
        export MASTER_PORT=$RANDOM_PORT
        break
    fi
done

echo "Using Master Port: $MASTER_PORT"

# 5. DeepSpeed ì‹¤í–‰ ëª…ë ¹ì–´
# ğŸš€ ìµœì í™” í¬ì¸íŠ¸:
# - All Linear Layers ì ìš© (q,k,v,o,gate,up,down)
# - Total Batch Size 128ë¡œ ë§ì¶¤ (1 GPU * 2 BS * 64 Acc = 128) 
# - ë§Œì•½ GPU 2ê°œë¥¼ ì“´ë‹¤ë©´ --include=localhost:0,1 ë¡œ ë°”ê¾¸ê³  Accë¥¼ 32ë¡œ ë‚®ì¶”ì„¸ìš”.
deepspeed --master_port=$MASTER_PORT --include=localhost:3 train.py \
  --deepspeed configs/ds_config_zero2_no_offload.json \
  --full_finetune False \
  --model_name_or_path $BASE_MODEL \
  --seed $SEED \
  --data_seed $SEED \
  --base_dtype $BASE_DTYPE \
  --adapter_dtype $ADAPTER_DTYPE \
  --init_weights lava \
  --lora_rank $RANK \
  --lora_alpha $ALPHA \
  --lora_dropout 0 \
  --target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \
  --data_path $DATA_PATH \
  --sub_task conversation \
  --dataset_split train \
  --dataset_field instruction output \
  --output_dir $OUTPUT_PATH \
  --num_train_epochs 1 \
  --model_max_length 256 \
  --per_device_train_batch_size 4 \
  --gradient_checkpointing True \
  --gradient_accumulation_steps 32 \
  --learning_rate 1e-4 \
  --warmup_ratio 0.03 \
  --lr_scheduler_type cosine \
  --logging_steps 10 \
  --save_strategy steps \
  --save_steps 1000 \
  --save_total_limit 1 \
  --report_to wandb \
  --optim adamw_torch \
  --merge False \
  --lambda_vib $LAMBDA_VIB \
  --lambda_stab $LAMBDA_STAB \
  --lambda_latent_stability 1.0 