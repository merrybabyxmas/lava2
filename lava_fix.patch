--- train.py.backup
+++ train.py
@@ -260,17 +260,29 @@
             model = get_peft_model(model, peft_config)
             script_args.merge = False
             
-            # ğŸ”¥ LAVA ì „ìš© ì²˜ë¦¬: 1. ì „ë¶€ freeze
-            for _, p in model.named_parameters():
-                p.requires_grad = False
-
-            # ğŸ”¥ LAVA ì „ìš© ì²˜ë¦¬: 2. LAVA moduleë§Œ, ê·¸ë¦¬ê³  float íŒŒë¼ë¯¸í„°ë§Œ trainable
-            for module in model.modules():
-                if module.__class__.__name__.lower().startswith("lava"):
-                    for p in module.parameters():
-                        if p.dtype.is_floating_point:
-                            p.requires_grad = True
-   
+            # ğŸ”¥ LAVA ì „ìš© ì²˜ë¦¬: parameter name ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
+            logger.info("Setting LAVA parameters to trainable...")
+            
+            trainable_params = []
+            for name, param in model.named_parameters():
+                # LAVA íŒŒë¼ë¯¸í„°ë§Œ trainableë¡œ ì„¤ì •
+                if 'lava' in name.lower():
+                    if param.dtype.is_floating_point:
+                        param.requires_grad = True
+                        trainable_params.append(name)
+                        
+                        # ğŸ”¥ Device ì´ë™
+                        if param.device.type == 'cpu':
+                            param.data = param.data.to('cuda:0')
+                    else:
+                        param.requires_grad = False
+                else:
+                    # Non-LAVA íŒŒë¼ë¯¸í„°ëŠ” freeze
+                    param.requires_grad = False
+            
+            if script_args.local_rank == 0:
+                logger.info(f"Set {len(trainable_params)} LAVA parameters to trainable")
+                logger.info(f"Sample trainable params: {trainable_params[:5]}")
         else:
             logger.info(f'Init LoRA/PiSSA modules...')
             peft_config = LoraConfig(
