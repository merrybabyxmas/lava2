import torch
from transformers import AutoModelForCausalLM
from peft import PeftModel

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_4bit=True,
    device_map="auto"
)

model = PeftModel.from_pretrained(
    model,
    "output/metamath-LAVA-Llama-2-7b-r128-seed42-4bit/checkpoint-2000/adapter_model"
)

print("=" * 60)
print("œÉ (Standard Deviation) Analysis")
print("=" * 60)

sigmas = []
for name, param in model.named_parameters():
    if 'b_logvar' in name:
        sigma = torch.exp(0.5 * param.data)
        sigmas.append(sigma)
        print(f"{name[:50]}...")
        print(f"  mean: {sigma.mean():.4f}")
        print(f"  std:  {sigma.std():.4f}")
        print(f"  min:  {sigma.min():.4f}")
        print(f"  max:  {sigma.max():.4f}")

all_sigmas = torch.cat([s.flatten() for s in sigmas])
print(f"\nOverall Statistics:")
print(f"  Mean œÉ: {all_sigmas.mean():.4f}")
print(f"  Std œÉ:  {all_sigmas.std():.4f}")

# Ìï¥ÏÑù
if all_sigmas.mean() < 0.1:
    print("\n‚úÖ œÉÍ∞Ä ÏûëÏùå ‚Üí Training-Inference gap ÏûëÏùÑ Í≤É")
elif all_sigmas.mean() < 0.3:
    print("\n‚ö†Ô∏è œÉÍ∞Ä Ï§ëÍ∞Ñ ‚Üí ÏïΩÍ∞ÑÏùò gap ÏòàÏÉÅ")
else:
    print("\nüö® œÉÍ∞Ä ÌÅº ‚Üí Training-Inference gap Ïã¨Í∞ÅÌï† Ïàò ÏûàÏùå")
